"""Module for testing Conditional Value at Risk (CVaR) model functionality.

This module contains tests for the CVaR model components of the project,
including target generation and model optimization. It verifies that the
model produces expected allocations, portfolio values, and risk metrics.
"""

from datetime import timedelta

import numpy as np
import pandas as pd
import pytest

from ifunnel.models.cvar_model import cvar_model
from ifunnel.models.cvar_targets import get_cvar_targets


@pytest.fixture(scope="module")
def n_simulations_target():
    """Provide the number of simulations to use for CVaR target generation.

    This fixture returns a fixed number of simulations to ensure consistent
    test results across test runs.

    Returns:
        int: Number of simulations for scenario generation.
    """
    return 250


@pytest.fixture()
def cvar_target_data(start_test_date, weekly_returns, scgen, n_simulations_target, request):
    """Generate CVaR targets and benchmark portfolio values for testing.

    This fixture calls the get_cvar_targets function with the specified benchmark
    (provided via parametrization) to generate targets and benchmark portfolio values
    for testing the CVaR model.

    Args:
        start_test_date: Start date for the test dataset.
        weekly_returns: DataFrame containing weekly returns data.
        scgen: ScenarioGenerator instance for generating scenarios.
        n_simulations_target: Number of simulations to use.
        request: Pytest request object for accessing parametrized values.

    Returns:
        tuple: A tuple containing:
            - targets: DataFrame with CVaR targets.
            - benchmark_port_val: DataFrame with benchmark portfolio values.
    """
    start_of_test_dataset = str(start_test_date + timedelta(days=1))
    targets, benchmark_port_val = get_cvar_targets(
        test_date=start_of_test_dataset,
        benchmark=request.getfixturevalue(request.param),  # MSCI World benchmark
        budget=100,
        cvar_alpha=0.05,
        data=weekly_returns,
        scgen=scgen,
        n_simulations=n_simulations_target,
    )
    return targets, benchmark_port_val


@pytest.fixture(scope="module")
def cvar_dir(resource_dir):
    """Provide the path to the CVaR test resources directory.

    This fixture returns the path to the directory containing CVaR test
    resources like baseline files for comparison.

    Args:
        resource_dir: Path to the main test resources directory.

    Returns:
        Path: Path to the CVaR test resources directory.
    """
    return resource_dir / "cvar"


@pytest.mark.parametrize(
    "cvar_target_data, label",
    [("benchmark_isin_1", "1"), ("benchmark_isin_2", "2")],
    indirect=["cvar_target_data"],
)
def test_get_cvar_targets(cvar_target_data, label, cvar_dir):
    """Test the CVaR target generation functionality.

    This test verifies that the CVaR targets and benchmark portfolio values
    generated by the get_cvar_targets function match the expected baseline values.
    The test is parametrized to run with different benchmark portfolios.

    Args:
        cvar_target_data: Fixture providing the generated targets and benchmark values.
        label: Label identifying the benchmark portfolio being tested.
        cvar_dir: Path to the directory containing CVaR test resources.
    """
    expected_targets = pd.read_csv(cvar_dir / f"targets_{label}_BASE.csv", index_col=0)
    expected_benchmark_port_val = pd.read_csv(
        cvar_dir / f"benchmark_port_val_{label}_BASE.csv",
        index_col=0,
        parse_dates=True,
    )
    expected_benchmark_port_val.index = expected_benchmark_port_val.index.astype("datetime64[us]")

    targets, benchmark_port_val = cvar_target_data

    # targets.to_csv(cvar_dir / f"targets_{label}_BASE.csv")
    # benchmark_port_val.to_csv(cvar_dir / f"benchmark_port_val_{label}_BASE.csv")
    pd.testing.assert_frame_equal(targets, expected_targets)
    pd.testing.assert_frame_equal(benchmark_port_val, expected_benchmark_port_val)


@pytest.mark.parametrize("cvar_target_data", ["benchmark_isin_2"], indirect=True)
def test_cvar_model(test_narrow_dataset, mc_scenarios, cvar_target_data, resource_dir, cvar_dir):
    """Test the CVaR model optimization functionality.

    This test verifies that the CVaR model produces the expected portfolio allocations,
    values, and risk metrics when run with the CLARABEL solver. It compares the model
    outputs against baseline files and checks that active constraints are satisfied.

    Args:
        test_narrow_dataset: DataFrame containing test return data.
        mc_scenarios: Monte Carlo scenarios for the optimization.
        cvar_target_data: Fixture providing CVaR targets and benchmark values.
        resource_dir: Path to the main test resources directory.
        cvar_dir: Path to the CVaR test resources directory.
    """
    expected_port_allocation = pd.read_csv(cvar_dir / "port_allocation_CLARABEL.csv", index_col=0)
    expected_port_value = pd.read_csv(cvar_dir / "port_value_CLARABEL.csv", index_col=0, parse_dates=True)
    expected_port_cvar = pd.read_csv(cvar_dir / "port_cvar_CLARABEL.csv", index_col=0)

    # np.savez_compressed("scgen/scenarios_BASE.npz", scenarios=mc_scenarios)
    generated_scenarios = np.load(resource_dir / "scgen/scenarios_BASE.npz")["scenarios"]

    targets, _ = cvar_target_data

    port_allocation, port_value, port_cvar = cvar_model(
        test_ret=test_narrow_dataset,
        scenarios=generated_scenarios,  # Scenarios
        targets=targets,  # Target
        budget=100,
        cvar_alpha=0.05,
        trans_cost=0.001,
        max_weight=1,
        solver="CLARABEL",
        lower_bound=0,
    )

    # Uncomment to generate new baseline files with CLARABEL solver
    # port_allocation.to_csv(cvar_dir / "port_allocation_CLARABEL.csv")
    # port_value.to_csv(cvar_dir / "port_value_CLARABEL.csv")
    # port_cvar.to_csv(cvar_dir / "port_cvar_CLARABEL.csv")

    active_constraints = (targets.to_numpy() - port_cvar.to_numpy()) < 1e-5

    # Ensure both DataFrames have the same columns in the same order
    all_columns = sorted(set(port_allocation.columns) | set(expected_port_allocation.columns))
    port_allocation = port_allocation.reindex(columns=all_columns).fillna(0)
    expected_port_allocation = expected_port_allocation.reindex(columns=all_columns).fillna(0)

    # Ensure both DataFrames have the same indices for port_value
    port_value = port_value.reindex(expected_port_value.index)

    # Ensure both DataFrames have the same indices for port_cvar
    port_cvar_active = port_cvar[active_constraints].reindex(expected_port_cvar[active_constraints].index)
    expected_port_cvar_active = expected_port_cvar[active_constraints]

    pd.testing.assert_frame_equal(port_allocation, expected_port_allocation, atol=1e-5)
    pd.testing.assert_frame_equal(port_value, expected_port_value)
    pd.testing.assert_frame_equal(port_cvar_active, expected_port_cvar_active)
